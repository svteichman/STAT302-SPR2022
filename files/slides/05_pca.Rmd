---
title: "STAT 302 Lecture Slides 5"
subtitle: "Multivariate Data Analysis and PCA"
author: "Sarah Teichman (adapted from slides by Bryan Martin and Peter Gao)"
date: ""
output:
  xaringan::moon_reader:
    css: ["default", "gao-theme.css", "gao-theme-fonts.css"]
    nature:
      highlightStyle: default
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["center"]
---

```{r setup, include=FALSE, purl=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(comment = "##")
knitr::opts_chunk$set(cache = TRUE)
library(kableExtra)
library(tidyverse)
```


# Today's pet picture 


```{r, echo = FALSE, out.width = "350px", fig.align = "center"}
knitr::include_graphics("../../files/images/pets/orange_cat.jpg")
```

Thanks Esther! 

---

# Outline

1. Multivariate data and dimension reduction 
2. Principal Components Analysis (PCA): Theory & Application


.middler[**Goal:** Practice using R to implement a statistical method to analyze multivariate data.]

---

class: inverse

.sectionhead[Part 1. Multivariate Data]

---

# Multivariate Data 

* Multi -> Multiple

* Variate -> Variables 

* Multivariate -> Multiple Variables 

--

```{r, message = FALSE}
data(mtcars)
library(tidyverse)
library(gapminder)
data(gapminder)
names(mtcars)
names(gapminder)
```

---

# Multivariate Data 

How have we analyzed multivariate data so far in our class? 

```{r}
summary(gapminder$lifeExp)
```

```{r, echo = FALSE, fig.align = 'center', fig.height = 5}
ggplot(gapminder, aes(x = lifeExp)) + 
  geom_histogram(bins = 20, fill = "lightblue", color = "black") + 
  ggtitle("Histogram of Life Expectancy") + 
  xlab("Life Expectancy (in years)") + 
  ylab("Count") + 
  theme_bw(base_size = 14) + 
  theme(plot.title = element_text(hjust = 0.5))
```

---

# Multivariate Data 

How have we analyzed multivariate data so far in our class? 

```{r}
cor(gapminder$lifeExp, gapminder$gdpPercap)
```

```{r, echo = FALSE, fig.align = 'center', fig.height = 5}
ggplot(gapminder, aes(x = lifeExp, y = log(gdpPercap))) + 
  geom_point() + 
  ggtitle("Logged GDP per Capita by Life Expectancy") + 
  xlab("Life Expectancy") + 
  ylab("Logged GDP per capita") + 
  theme_bw(base_size = 14) + 
  theme(plot.title = element_text(hjust = 0.5))
```

---

# Multivariate Data 

What if we want a way to look at all our variables at once? 

--

What if we want our analysis to consider the relationships between observations based on all the variables in our data, not just one or two? 

---

# Multivariate Data 

What if we want a way to look at all our variables at once? 

```{r, message = FALSE, echo = FALSE, fig.align='center', warning=FALSE}
library(GGally)
ggpairs(gapminder, columns = 4:ncol(gapminder))
```

---

# Multivariate Data 

What if we want a way to look at all our variables at once?


--


And we have more than a handful of variables?


---


# Multivariate Data 

Dimension reduction: A transformation from a high-dimensional space into a low-dimensional space so that the low-dimensional space retains some meaningful properties of the original data 

--

If we only consider quantitative variables, `gapminder` can be represented in 4 dimensions: 

```{r}
str(gapminder)
```

---

# Multivariate Data

Dimension reduction: A transformation from a high-dimensional space into a low-dimensional space so that the low-dimensional space retains some meaningful properties of the original data 


Consider this dataset with $29$ indicators of environmental performance for $180$ countries.

```{r, echo = FALSE}
environ_df <- read.csv("../projects/02_dimension_reduction/pca_data.csv")
names(environ_df[2:ncol(environ_df)])
```


---

class: inverse

.sectionhead[Part 2. Principal Components Analysis (PCA): Theory & Application]

---

# Principal Components Analysis (PCA)


.middler[**Goal**: Construct a smaller set of variables that collectively retain meaningful properties of the original data set.]

---

# Principal Components Analysis (PCA)


.middler[**Goal**: Construct a smaller set of *uncorrelated* variables that collectively retain meaningful properties of the original data set.]

---

# Principal Components Analysis (PCA)


.middler[**Goal**: Construct a smaller set of *uncorrelated linear combinations of the original variables* that collectively retain meaningful properties of the original data set.]

---

# Principal Components Analysis (PCA)


.middler[**Goal**: Construct a smaller set of *uncorrelated linear combinations of the original variables* that collectively retain *most of the variability* of the original data set.]


---

# Principal Components Analysis (PCA)

**Goal**: Construct a smaller set of *uncorrelated linear combinations of the original variables* that collectively retain *most of the variability* of the original data set.

* Principal components: *uncorrelated linear combinations of the original variables*

--

* Principal components are ordered by how much of the variability of the original data they explain

--

* Intuition: If we have $p$ original variables but we can explain most of the variability in our data with $k$ (if $k < p$) principal components, then we can reduce the dimension of our dataset while retaining most of the data's variability 

---

# Principal Components Analysis (PCA)

**Goal**: Construct a smaller set of *uncorrelated linear combinations of the original variables* that collectively retain *most of the variability* of the original data set.

* Define our original variables $\mathbf{x}_1,\ldots, \mathbf{x}_p$, where each $\mathbf{x}_j$ is a vector of length $n$ representing $n$ observations of variable $j$

--


* Our principal components are linear combinations of our (centered) data vectors $\mathbf{x}^*_1,\ldots, \mathbf{x}^*_p$:

$$\mathbf{y}_i = l_{i1}\mathbf{x}^*_1 + l_{i2}\mathbf{x}^*_2 + \ldots + l_{ip}\mathbf{x}^*_p$$
--

* We call these scaling values $l_{ij}$ loadings. 

---

# Principal Components Analysis (PCA)

## Principal Component 1 

* For our first principal component, we want to choose $l_{11},\ldots,l_{1p}$ in order to maximize the variance of $\mathbf{y}_1$.

$$\text{maximize}_{l_{11},\ldots,l_{1p}}\ var(\mathbf{y}_1)$$
$$\text{maximize}_{l_{11},\ldots,l_{1p}}\ var(l_{11}\mathbf{x}^*_1 + \ldots + l_{1p}\mathbf{x}^*_p)$$
--

* However, we could always increase the variance of $\mathbf{y}_1$ by just increasing the values of $\mathbf{l}_1$. Let's add the constraint that $\sum_{j=1}^p l_{1j}^2 = 1$. 

$$\text{maximize}_{l_{11},\ldots,l_{1p}}\ var(l_{11}\mathbf{x}^*_1 + \ldots + l_{1p}\mathbf{x}^*_p)\ \text{s.t.}\  \sum_{j=1}^p l_{1j}^2 = 1$$

---

# Principal Component 2 


* Let's start with the same goal as the first principal component: 

$$\text{maximize}_{l_{21},\ldots,l_{2p}}\ var(l_{21}\mathbf{x}^*_1 + \ldots + l_{2p}\mathbf{x}^*_p)\ \text{s.t.}\  \sum_{j=1}^p l_{2j}^2 = 1$$
--

* Wait, what about the uncorrelated part? 

$$\text{maximize}_{l_{21},\ldots,l_{2p}}\ var(l_{21}\mathbf{x}^*_1 + \ldots + l_{2p}\mathbf{x}^*_p)\ \text{s.t.}\  \sum_{j=1}^p l_{2j}^2 = 1,\ \sum_{j=1}^p l_{1j}l_{2j} = 0$$
---

# A Brief Geometric Interpretation 

* Let's pretend that we only have two original vectors, $\mathbf{x}_1$ and $\mathbf{x}_2$. 

* First, we want to identify the axes along which the data have the most variance.

.center[<img src="images/pcagif.gif" alt="" height="375"/>]

.footnote[[Source on stackexchange](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)]


---

# A Brief Geometric Interpretation 

* Let's pretend that we only have two original vectors, $\mathbf{x}_1$ and $\mathbf{x}_2$. 

* Next, we want to find an uncorrelated vector that explains as much of the remaining variance as possible. 

.center[<img src="images/pca2plot.svg" alt="" height="375"/>]


.footnote[[Source on Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)]

---

# The Other Principal Components 

* We can write the same maximization problem for each of the $p$ principal components, adding additional constraints to make sure each component is uncorrelated with all the previous ones. 

---

# Example of PCA 

Let's use the five quantitative variables from the `mtcars` dataset. We'll use the R function `princomp()`. 

```{r}
cars_pca_df <- mtcars %>% 
  select(mpg, disp, drat, wt, qsec)
cars_pca <- princomp(cars_pca_df)
cars_pca$loadings
```

---

# Example of PCA

Let's look closer at the loadings on the first two principal components. 

```{r, echo = FALSE, fig.align='center'}
plot_df <- data.frame(loading1 = cars_pca$loadings[ ,1],
                      loading2 = cars_pca$loadings[, 2],
                      name = names(cars_pca_df))
ggplot(plot_df, aes(x = loading1,
                    y = loading2,
                    label = name)) + 
  geom_point() + 
  geom_text(nudge_x = 0.06) + 
  geom_hline(yintercept = 0, color = "gray") + 
  geom_vline(xintercept = 0, color = "gray") +
  xlab("Loadings for Principal Component 1") + 
  ylab("Loadings for Principal Component 2") + 
  ggtitle("Loadings for First Two Principal Components") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = element_text(hjust = 0.5))
```

---

# Example of PCA

Let's look closer at the loadings. 

.pull-left[
```{r, echo = FALSE, fig.align='center'}
plot_df <- data.frame(loading1 = cars_pca$loadings[ ,1],
                      loading2 = cars_pca$loadings[, 2],
                      name = names(cars_pca_df))
ggplot(plot_df, aes(x = loading1,
                    y = loading2,
                    label = name)) + 
  geom_point() + 
  geom_text(nudge_x = 0.06) + 
  geom_hline(yintercept = 0, color = "gray") + 
  geom_vline(xintercept = 0, color = "gray") +
  xlab("Loadings for Principal Component 1") + 
  ylab("Loadings for Principal Component 2") + 
  ggtitle("Loadings for First Two Principal Components") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = element_text(hjust = 0.5))
```
]

.pull-right[
```{r}
apply(cars_pca_df, 2, range)
```
]

---

# Scaling Variables before PCA 

* Because of this behavior, where principal components depend on the scale of the data, we often scale our variables before performing PCA.
* To scale a variable $\mathbf{x}_j$, first take the centered variable $\mathbf{x}^*_j$, then divide by the standard deviation of variable $j$, to get the centered and scaled version of variable $i$, $\tilde{\mathbf{x}}_j$.



---

# Back to our example 

```{r}
cars_pca_df <- mtcars %>% 
  select(mpg, disp, drat, wt, qsec)
cars_pca_scaled <- princomp(cars_pca_df, cor = TRUE)
cars_pca_scaled$loadings
```

---

# Back to our example

Let's look closer at the loadings on the first two principal components. 

```{r, echo = FALSE, fig.align='center'}
plot_df <- data.frame(loading1 = cars_pca_scaled$loadings[ ,1],
                      loading2 = cars_pca_scaled$loadings[, 2],
                      name = names(cars_pca_df))
loading_plot <- ggplot(plot_df, aes(x = loading1,
                    y = loading2,
                    label = name)) + 
  geom_point() + 
  geom_hline(yintercept = 0, color = "gray") + 
  geom_vline(xintercept = 0, color = "gray") +
  geom_text(nudge_x = 0.06) + 
  xlab("Loadings for Principal Component 1") + 
  ylab("Loadings for Principal Component 2") + 
  ggtitle("Loadings for First Two Principal Components") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = element_text(hjust = 0.5))
loading_plot
```

---

# Wait, what about dimension reduction? 

* Remember, the whole point of PCA is to construct a set of *uncorrelated linear combinations of the original variables* that collectively retain *most of the variability* of the original data set.

--

* Our principal components are ordered by the amount of variability they can explain. 

--

* Intuition: If we have $p$ original variables but we can explain most of the variability in our data with $k$ (if $k < p$) principal components, then we can reduce the dimension of our dataset while retaining most of the data's variability. 


---

# How many principal components to use? 

* We'd like to choose a value $k$ that is small enough to reduce the dimension substantially and big enough to explain much of the data's variability. 

---

# How many principal components to use? 

* We can make a scree plot to visualize the proportion of variance explained by each principal component. 

```{r, fig.align='center', fig.height = 5}
screeplot(cars_pca_scaled, type = "lines", main = "Scree Plot")
```

* In a scree plot, we look for the "elbow."

---

# How many principal components to use? 

* In practice, we often look primarily at the first two principal components, because we can plot these in two dimensions. 

.pull-left[
```{r, eval = FALSE}
plot_df <- 
  data.frame(pc1 = cars_pca_scaled$
               scores[, 1],
             pc2 = cars_pca_scaled$
               scores[, 2],
             name = rownames(mtcars))
ggplot(plot_df, 
       aes(x = pc1, y = pc2, 
           label = name)) + 
  geom_point() + 
  geom_text(size = 4, 
            nudge_x = .12, 
            nudge_y = .12) + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2",
       title = "First Two Principal Components of mtcars") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = 
          element_text(hjust = 0.5))
```
]

.pull-right[
```{r, echo = FALSE}
plot_df <- 
  data.frame(pc1 = cars_pca_scaled$scores[, 1],
             pc2 = cars_pca_scaled$scores[, 2],
             name = rownames(mtcars))
ggplot(plot_df, 
       aes(x = pc1, y = pc2, label = name)) + 
  geom_point() + 
  geom_text(size = 4, 
            nudge_x = .12, nudge_y = .12) + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2",
       title = "First Two Principal Components of mtcars") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = element_text(hjust = 0.5))
```
]

---

# How many principal components to use? 

* In practice, we often look primarily at the first two principal components, because we can plot these in two dimensions. 

.pull-left[
```{r, eval = FALSE}
text_df <- plot_df %>%
  filter(name %in%
           c("Merc 230", "Hornet 4 Drive",
             "Lotus Europa", "Porsche 914-2",
             "Maserati Bora", "Ferrari Dino"))
ggplot(plot_df, 
       aes(x = pc1, y = pc2,
           label = name)) + 
  geom_point() + 
  geom_text(data = text_df,
            size = 4, 
            nudge_x = .12, 
            nudge_y = .12) + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2",
       title = "First Two Principal Components of mtcars") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = 
          element_text(hjust = 0.5))
```
]

.pull-right[
```{r, echo = FALSE}
text_df <- plot_df %>%
  filter(name %in%
           c("Merc 230", "Hornet 4 Drive",
             "Lotus Europa", "Porsche 914-2",
             "Maserati Bora", "Ferrari Dino"))
pc2_plot <- ggplot(plot_df, 
       aes(x = pc1, y = pc2,
           label = name)) + 
  geom_point() + 
  geom_text(data = text_df,
            size = 4, 
            nudge_x = .12, 
            nudge_y = .12) + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2",
       title = "First Two Principal Components of mtcars") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = 
          element_text(hjust = 0.5))
pc2_plot
```
]

---

# What does this plot tell us?

* Plotting our observations on the first two principal components lets us compare them 

--

* Which observations are similar to each other? Which are different?

--

* Are there certain patterns or trends we can see in this low dimensional space? 

---

# Biplots 

* Biplots show the observations and loadings for the first two principal components. 

```{r, fig.align='center', fig.height = 6}
biplot(cars_pca_scaled)
```

---

# Biplots 

.pull-left[
```{r, echo = FALSE}
loading_plot
```
]

.pull-right[
```{r, echo = FALSE}
pc2_plot
```
]

---

# Back to first two principal components 

```{r, echo = FALSE, fig.align='center'}
plot_df <- 
  data.frame(pc1 = cars_pca_scaled$scores[, 1],
             pc2 = cars_pca_scaled$scores[, 2],
             name = rownames(mtcars),
             transmission = mtcars$am)
ggplot(plot_df, 
       aes(x = pc1, y = pc2, 
           label = name,
           color = as.factor(transmission))) + 
  geom_point(size = 3) + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2",
       color = "Transmission",
       title = "First Two Principal Components of mtcars") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = 
          element_text(hjust = 0.5))
```

---

# Back to first two principal components 

```{r, eval = FALSE, fig.align='center'}
plot_df <- 
  data.frame(pc1 = cars_pca_scaled$scores[, 1],
             pc2 = cars_pca_scaled$scores[, 2],
             name = rownames(mtcars),
             transmission = mtcars$am)
ggplot(plot_df, 
       aes(x = pc1, y = pc2, 
           label = name,
           color = as.factor(transmission))) + 
  geom_point(size = 3) + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2",
       color = "Transmission",
       title = "First Two Principal Components of mtcars") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = 
          element_text(hjust = 0.5))
```

---

# Back to first two principal components 

```{r, echo = FALSE, fig.align='center'}
plot_df <- 
  data.frame(pc1 = cars_pca_scaled$scores[, 1],
             pc2 = cars_pca_scaled$scores[, 2],
             name = rownames(mtcars),
             cylinders = mtcars$cyl)
ggplot(plot_df, 
       aes(x = pc1, y = pc2, 
           label = name,
           color = as.factor(cylinders))) + 
  geom_point(size = 3) + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2",
       color = "Cylinders",
       title = "First Two Principal Components of mtcars") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = 
          element_text(hjust = 0.5))
```

---

# Back to first two principal components 

```{r, eval = FALSE, fig.align='center'}
plot_df <- 
  data.frame(pc1 = cars_pca_scaled$scores[, 1],
             pc2 = cars_pca_scaled$scores[, 2],
             name = rownames(mtcars),
             cylinders = mtcars$cyl)
ggplot(plot_df, 
       aes(x = pc1, y = pc2, 
           label = name,
           color = as.factor(cylinders))) + 
  geom_point(size = 3) + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2",
       color = "Cylinders",
       title = "First Two Principal Components of mtcars") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = 
          element_text(hjust = 0.5))
```

---

# Back to first two principal components 

```{r, echo = FALSE, fig.align='center'}
plot_df <- 
  data.frame(pc1 = cars_pca_scaled$scores[, 1],
             pc2 = cars_pca_scaled$scores[, 2],
             name = rownames(mtcars),
             hp = mtcars$hp)
ggplot(plot_df, 
       aes(x = pc1, y = pc2, 
           label = name,
           color = hp)) + 
  geom_point(size = 3) + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2",
       color = "Horse Power",
       title = "First Two Principal Components of mtcars") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = 
          element_text(hjust = 0.5)) + 
  scale_color_continuous(low = "blue", high = "red")
```

---

# Back to first two principal components 

```{r, eval = FALSE, fig.align='center'}
plot_df <- 
  data.frame(pc1 = cars_pca_scaled$scores[, 1],
             pc2 = cars_pca_scaled$scores[, 2],
             name = rownames(mtcars),
             hp = mtcars$hp)
ggplot(plot_df, 
       aes(x = pc1, y = pc2, 
           label = name,
           color = hp)) + 
  geom_point(size = 3) + 
  labs(x = "Principal Component 1", 
       y = "Principal Component 2",
       color = "Horse Power",
       title = "First Two Principal Components of mtcars") + 
  theme_bw(base_size = 12) + 
  theme(plot.title = 
          element_text(hjust = 0.5)) + 
  scale_color_continuous(low = "blue", high = "red")
```


